{
 "cells": [
  {
   "cell_type": "raw",
   "id": "3d2fe535",
   "metadata": {},
   "source": [
    "\n",
    "1.Linear Regration Co_relation\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "Functions>>1>> df=pd.read_csv(path/name of file) >> for reading csv files which we have available\n",
    "           4>> df.info()\n",
    "           5>> df.describe()\n",
    "           2>> df.head(number)>> for taking number of observations\n",
    "           3>> df.drop(['column name1','column name2'],axis=1,inplace=True)>>for droping number of columns and save in original dataframe\n",
    "           4>> sns.pairplot(df)>>for getting co-relation of two independent variables in the form graphical representation>>for checking linearity.\n",
    "           8>> sns.scatterplot(x=df['SepalLengthCm'],y=df['SepalWidthCm'])>>for checking linearity.and its shows outliers as well.\n",
    "           5>> df.corr()>>for getting co-relation values in between independent variables.\n",
    "           6>> sns.heatmap(df.corr())>>for get box like structure and according to value we get boxes colour changes.\n",
    "           7>> sns.heatmap(df.corr(),annot=True)>>for get box like structure and according to value we get boxes colour and we get inside of that boxes coeficient of corelation value in round figure.\n",
    "\n",
    "2.No Multicolinearity>> we get boston name bcz our file name is boston\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "\n",
    "from sklearn.datasets import load_iris,load_boston\n",
    "\n",
    "Functions>>1.Problem Statement\n",
    "           2.Data Gathering>>1.boston=boston.load >> the file we are getting from directly sklearn datasets\n",
    "                             2.boston.feature_names>>we get only all indipendent variables columns names\n",
    "                             3.boston_df=pd.DataFrame(boston.data,columns=boston.feature_names)>>we get data and column name in data frame\n",
    "                             boston_df['PRICE']=boston.target>>we add price as a target variable in data frame\n",
    "                             boston_df\n",
    "           3.Explorarory Data Analysis>>\n",
    "                             1.boston_df.info() >> check all information\n",
    "                             2.boston_df.describe()>> get all numeric values\n",
    "                             3.sns.pairplot(boston_df) \n",
    "                             4.sns.heatmap(boston_df.corr(),annot=True)\n",
    "                             5.plt.figure(figsize=(row_num,columns_num))>> we can give shape for that boxes rows and columns.\n",
    "                             6.dir(boston)>> we se what is inside in that data\n",
    "                             7.boston.DESCR >> description\n",
    "                             8.boston.filename \n",
    "                             9.plt.figure(figsize=(20,20))>> for getting 20x20 rows and columns numbers.\n",
    "                               sns.heatmap(boston_df.corr(),annot=True, cmap='YlGnBu') >> cmap uses here bcz we can get different colourse for our boxes.\n",
    "                            10.plt.figure(figsize=(20,20))\n",
    "                               sns.heatmap(boston_df.corr(),annot=True, cmap='rainbow') >> rainbow coloure\n",
    "                            11.No Multicolinearity>>1.boston_df.columns >> all columns names \n",
    "                                                    2.df1=boston_df.drop('PRICE',axis=1)>> droping price column\n",
    "3.vif_list=[]\n",
    "for i in range(df1.shape[1]):\n",
    "    vif=variance_inflation_factor(df1.to_numpy(),i)\n",
    "    vif_list.append(vif)\n",
    "S1=pd.Series(vif_list,index=df1.columns)\n",
    " # S1.plot(kind='barh')\n",
    "    # or\n",
    "S1.sort_values().plot(kind='barh')\n",
    "                                                    4.df1.shape>> we can see rows and columns\n",
    "                                                    5.df1.to_numpy()>>converting in array\n",
    "                                                    \n",
    "         4.Data Splitting>>1.x=boston_df.drop('PRICE',axis=1)>> target column\n",
    "                             y=boston_df['PRICE']>> only one column target\n",
    "                             print(x,y)\n",
    "                           2.x_train,x_test,y_train,y_test = train_test_split(x,y,test_size=0.2,random_state=1)>>test size means we giving 20% data for testing and random state=1 means we are randomaly shufling the data one time bcz we gave 1 number so                       \n",
    "                           3.model Training >> 1.boston_train=LinearRegression()\n",
    "                                                 boston_train.fit(x_train,y_train)\n",
    "                           4.model evaluation >> y_pred = boston_tarin.predict(x_test))\n",
    "                           \n",
    "                           evaluation on training data>>\n",
    "                           \n",
    "                           1.mse= mean_squared_error(y_train,y_pred)\n",
    "                           2.rmse= np.sqrt(mse)\n",
    "                           3.mae= mean_absolute_error(y_train,y_pred)\n",
    "                           4.R2=r2_score(y_train,y_pred)\n",
    "                           5.adj_R2= 1-((1-R2)*(x_train.shape[0]-1)/(x_train.shape[0]-13-1))\n",
    "                           \n",
    "         5.Normality Of Residual>>\n",
    "                           1.Residual=Ya-Yp\n",
    "                             Residual=y_train-y_pred_train\n",
    "                           2.sns.kdeplot(Residual)>> for kde plot it is in curve form we can say density curve.\n",
    "                             sns.kdeplot(Residual,fill=True)>> for kde plot inside space fill\n",
    "                           3.sns.displot(Residual)\n",
    "                           4.sns.displot(Residual,kind='kde')>> it is in bar graph form\n",
    "                           5.sns.kdeplot(boston_df['perticular_variable'])\n",
    "                           6.sns.displot(boston_df['perticular_variable'])\n",
    "                           \n",
    "mean=boston_df['AGE'].mean()\n",
    "print('Mean is:',mean)\n",
    "\n",
    "median=boston_df['AGE'].median()\n",
    "print('Median is:',median)\n",
    "\n",
    "mode=boston_df['AGE'].mode()\n",
    "print('Mode is:',mode)\n",
    "\n",
    "skew=boston_df['AGE'].skew()\n",
    "print('Skewness is:',skew)\n",
    "\n",
    "                          7.from scipy.stats import skew\n",
    "                          skewness=skew(boston_df['AGE'])\n",
    "                          print(skewness)\n",
    "                          8.QQ Plot(Quantile Quantile Plot)>>\n",
    "                          import statsmodels.api as sm\n",
    "                          1) sm.qqplot(Residual)>> Normal QQ Plot\n",
    "                          2) sm.qqplot(Residual,line='45') QQ Plot with angle 45\n",
    "                          3) sm.qqplot(Residual,line='45',fit=True) QQ Plot with angle 45 and fit means at both axis scale will same.\n",
    "                          4) sm.qqplot(df['perticular_variable']) >> only changes here are we have taken perticular independent variable.\n",
    "                          2) sm.qqplot(df['perticular_variable'],line='45')\n",
    "                          2) sm.qqplot(df['perticular_variable'],line='45',fit=True)\n",
    "\n",
    "       6.Statistical Tests for Normality(Hypothesis Testing)\n",
    "                          1)shapiro test>> from scipy.stats import shapiro\n",
    "_,p_val = shapiro(Residual)\n",
    "print('p value:',p_val)\n",
    "\n",
    "if p_val>=0.05:\n",
    "    print('Null Hypothesis is True')\n",
    "    print('Data is normally distributed')\n",
    "    \n",
    "else:\n",
    "    print('Alternate Hypothesis is true')\n",
    "    print('Data is not Normally distributed')\n",
    "    \n",
    "                         2)kstest>> from scipy.stats import kstest\n",
    "_,p_val = kstest(Residual,'norm')\n",
    "print('p value:',p_val)\n",
    "\n",
    "if p_val>=0.05:\n",
    "    print('Null Hypothesis is True')\n",
    "    print('Data is normally distributed')\n",
    "    \n",
    "else:\n",
    "    print('Alternate Hypothesis is true')\n",
    "    print('Data is not Normally distributed')\n",
    "    \n",
    "                        3)normal test>> from scipy.stats import normaltest\n",
    "_,p_val = normaltest(Residual)\n",
    "print('p value:',p_val)\n",
    "\n",
    "if p_val>=0.05:\n",
    "    print('Null Hypothesis is True')\n",
    "    print('Data is normally distributed')\n",
    "    \n",
    "else:\n",
    "    print('Alternate Hypothesis is true')\n",
    "    print('Data is not Normally distributed')\n",
    "    \n",
    "                            1.Scatterplot >>\n",
    "                              1>> sns.scatterplot(x=y_test,y=y_pred) >> for checking linierity,multicolinearity,homoscadasticity,normality of residuals\n",
    "                              \n",
    "      7.1 >> Model Testing >> \n",
    "      1.x_test.head(1)\n",
    "      2.x_test.columns\n",
    "      \n",
    "      7.2 >> saving or dumping the model >> \n",
    "      import pickle\n",
    "      with open('Linear_model.pkl', 'wb') as f:\n",
    "      pickle.dump(boston_tain,f)\n",
    "      \n",
    "CRIM =0.04932\n",
    "ZN = 33.0\n",
    "INDUS = 2.18\n",
    "CHAS = 0.0\n",
    "NOX = 0.472\n",
    "RM = 6.849\n",
    "AGE = 70.3\n",
    "DIS = 3.1827\n",
    "RAD = 7.0\n",
    "TAX = 222.0\n",
    "PTRATIO = 18.4\n",
    "B = 396.9\n",
    "LSTAT = 7.53\n",
    "\n",
    "test_array = np.array([CRIM, ZN, INDUS, CHAS, NOX, RM, AGE, DIS, RAD, TAX, PTRATIO, B, LSTAT], ndmin=2)\n",
    "\n",
    "def predicted_price(test_array):\n",
    "    price = boston_tain.predict(test_array)[0]\n",
    "    price = np.around(price,2)\n",
    "    \n",
    "    return price\n",
    "\n",
    "price = predicted_price(test_array)\n",
    "print(f'Predicted price is: {price}')\n",
    "      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f957de38",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
