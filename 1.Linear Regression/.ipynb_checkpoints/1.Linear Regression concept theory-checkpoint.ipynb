{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c3b04c20",
   "metadata": {},
   "source": [
    "# Libraries Importing"
   ]
  },
  {
   "cell_type": "raw",
   "id": "4b4423b7",
   "metadata": {},
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sklearn.metrics import mean_absolute_error,mean_squared_error,r2_score\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "\n",
    "from sklearn.datasets import load_boston "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ba4d4dd",
   "metadata": {},
   "source": [
    "# Linear Regression"
   ]
  },
  {
   "cell_type": "raw",
   "id": "7149b46b",
   "metadata": {},
   "source": [
    "1.Linear Regression is type of Supervised Machine Learning algorithm.\n",
    "\n",
    "2.this is very basic algorithm.\n",
    "\n",
    "3.this algorithm is initially highly used and highly studied as well.\n",
    "\n",
    "4.Linear regression is linear approach to modelling the relationship between independent and dependent variable.\n",
    "\n",
    "5.the main target of linear regression is to find out best fit line.\n",
    "\n",
    "6.Linear regression is an algorithm where we basically try to understand the linear relationship between dependent and one or more independent variable.\n",
    "\n",
    "7.Linear regression is a supervised machine learning algorithm used to find a linear\n",
    "relationship between a dependent (y) and one or more independent (x) variables, hence\n",
    "called linear regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e890213",
   "metadata": {},
   "source": [
    "# Linear"
   ]
  },
  {
   "cell_type": "raw",
   "id": "9fc914ad",
   "metadata": {},
   "source": [
    "it is based on line so word linear came here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "758fafe0",
   "metadata": {},
   "source": [
    "# regression"
   ]
  },
  {
   "cell_type": "raw",
   "id": "f3ad908e",
   "metadata": {},
   "source": [
    "it is used for regression problem so word regression is here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00d50396",
   "metadata": {},
   "source": [
    "## Line based concept"
   ]
  },
  {
   "cell_type": "raw",
   "id": "74cdb6ae",
   "metadata": {},
   "source": [
    "1.it is based on line.so when it comes to linear regression we basically have straight line.\n",
    "2.straight line is nothing but a linear equation that is why this algorithm is known as linear regression."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd0e76fd",
   "metadata": {},
   "source": [
    "## Types of Linear Regression"
   ]
  },
  {
   "cell_type": "raw",
   "id": "5656f503",
   "metadata": {},
   "source": [
    "Types of Linear Regression\n",
    "\n",
    "1. Simple Linear Regression:\n",
    "If a single independent variable is used to predict the value of a numerical\n",
    "dependent variable, then such a Linear Regression algorithm is called Simple\n",
    "Linear Regression.\n",
    "y = mx + c\n",
    "\n",
    "2. Multiple Linear regression:\n",
    "If more than one independent variable is used to predict the value of a\n",
    "numerical dependent variable, then such a Linear Regression algorithm is called\n",
    "Multiple Linear Regression.\n",
    "\n",
    "y = m1x1 + m2x2 + ... + mnxn + c\n",
    "y = Dependent Variable (Target Variable)\n",
    "x = Independent Variable (predictor Variable)\n",
    "c = intercept of the line (Gives an additional degree of freedom)\n",
    "m = Linear regression coefficient (scale factor to each input value"
   ]
  },
  {
   "cell_type": "raw",
   "id": "56bb42f2",
   "metadata": {},
   "source": [
    "Types Of Linear Regression:\n",
    "1.simple Linear Regression\n",
    "   1 dependent variable and 1 independent variable.\n",
    "2.Multiple Linear Regression\n",
    "   1 dependent variable more than 1 independent variable."
   ]
  },
  {
   "cell_type": "raw",
   "id": "b8abb241",
   "metadata": {},
   "source": [
    "Dependent variable should be in continuous or numerical form."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "767fb26a",
   "metadata": {},
   "source": [
    "## Simple Linear Regression"
   ]
  },
  {
   "cell_type": "raw",
   "id": "269baaa1",
   "metadata": {},
   "source": [
    "Simple Linear Regression\n",
    "\n",
    "y = mx + c >> linear equation for straight line.\n",
    "\n",
    "y >> dependent variable.\n",
    "x >> independent variable.\n",
    "m >> slope.\n",
    "c >> intercept/it is a constant means the value of y when \n",
    "x is 0"
   ]
  },
  {
   "cell_type": "raw",
   "id": "db35cea8",
   "metadata": {},
   "source": [
    "For Simple Linear Regression\n",
    "\n",
    "X, Y\n",
    "X -- Independent Variable\n",
    "Y -- Dependent variable\n",
    "While training we split the data\n",
    "Training >> 80% >> x_train, y_train\n",
    "Testing  >> 20% >> x_test\n",
    "\n",
    "x_train, x_test, y_train, y_test\n",
    "X\n",
    "Y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5dd6059",
   "metadata": {},
   "source": [
    "## Multiple Linear Regression"
   ]
  },
  {
   "cell_type": "raw",
   "id": "78065cc8",
   "metadata": {},
   "source": [
    " Multiple Linear Regression\n",
    "\n",
    "y = m1x1 + m2x2 + c >> linear equation for straight line.   >> 2 independent variables.\n",
    "\n",
    "y= m1x1 + m2x2 + ....... + mnxn + c                         >> n number of independent variables"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f556aebf",
   "metadata": {},
   "source": [
    "## best fit line"
   ]
  },
  {
   "cell_type": "raw",
   "id": "2efa8c96",
   "metadata": {},
   "source": [
    "best fit line >> the line for which we get the lowest value of sum of residual square will be our best fit line.\n",
    "                 or we can say the line which passes from maximum number of datapointes."
   ]
  },
  {
   "cell_type": "raw",
   "id": "42fd3e09",
   "metadata": {},
   "source": [
    "how x and y interact with each other and this interaction is nothing but our model. >> simply meaning that depend upon independent variable we get the dependent/target variable that is nothing but our prediction. so we says that x and y interact with each other."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f1aa4c0",
   "metadata": {},
   "source": [
    "## co-relation "
   ]
  },
  {
   "cell_type": "raw",
   "id": "0c44a148",
   "metadata": {},
   "source": [
    "co-relation : relation in between two variables."
   ]
  },
  {
   "cell_type": "raw",
   "id": "5cae44cd",
   "metadata": {},
   "source": [
    "to measure the strenght of the relationship between independent and target variable we use one parameter called coeficient of corelation(R).\n",
    "\n",
    "R=cov(x,y)/var(x).var(y)\n",
    "\n",
    "R=sum(xi-x^)(yi-y^)sqrt(sum(xi-x^)^2).sqrt(sum(yi-y^)^2)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "7b6833ca",
   "metadata": {},
   "source": [
    "coeficient of co-relation(R) values range is from -1 to 1.\n",
    " 0.7 to 1 >>    Good relation\n",
    "-0.7 to -1 >>   Good relation\n",
    "-0.3 to 0.3 >>  bad relation\n",
    "-0.3 to -0.7 >> not so good not so bad relation\n",
    " 0.3 to 0.7 >>  not so good not so bad relation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2b6853b",
   "metadata": {},
   "source": [
    "## Varience"
   ]
  },
  {
   "cell_type": "raw",
   "id": "4091735a",
   "metadata": {},
   "source": [
    "Varience: It is a measurement of the spread between nos. in the data set. \n",
    "    Varience measures variability of each data point from the average or mean.\n",
    "\n",
    "varience = (sum(xi-x^)^2)/n ....... value of i is from 1 to n\n",
    "co-varience = sum(xi-x^)(yi-y^)/n-1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5bfc3d4",
   "metadata": {},
   "source": [
    "# Assumptions"
   ]
  },
  {
   "cell_type": "raw",
   "id": "bf65e838",
   "metadata": {},
   "source": [
    "1.Linearity\n",
    "2.small/no multicolinearity\n",
    "3.Normality of residuals\n",
    "4.Homoscadasticity/No Heteroscadasticity"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cadaa46",
   "metadata": {},
   "source": [
    "# 1.Linearity"
   ]
  },
  {
   "cell_type": "raw",
   "id": "0791359d",
   "metadata": {},
   "source": [
    "1.Relationship in between independent and dependent variable should be linear.\n",
    "2.there should be a linear relationship in-between independent variable and dependent variable.\n",
    "3.It is a property of mathematical relationship that can be graphically represented as a staright line.\n",
    "4.we can check it with scatterplot."
   ]
  },
  {
   "cell_type": "raw",
   "id": "634b78cd",
   "metadata": {},
   "source": [
    "1. Linearity:\n",
    "    Relationship between the independent and dependent variables to be linear\n",
    "\n",
    "1.1.1 Coefficient of correlation(R): Correlation coefficients are used to\n",
    "measure how strong a relationship is between two variables.\n",
    "There are several types of correlation coefficients, but the most popular is\n",
    "Pearson’s.\n",
    "\n",
    "The range of R-Value is between -1 to +1\n",
    "R = 1 >> indicates a strong positive relationship. R = 1 >> indicates a strong positive\n",
    "relationship.\n",
    "R = -1 >> indicates a strong negative relationship. if one variable increases, other\n",
    "variable decreas\n",
    "R = 0 >> It means there is no linear relationship. It doesn’t mean that there is no\n",
    "relationship"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35ce37f3",
   "metadata": {},
   "source": [
    "## 1.1 How to check Linearity:"
   ]
  },
  {
   "cell_type": "raw",
   "id": "fc789889",
   "metadata": {},
   "source": [
    "1. Coefficient of correlation\n",
    "2. Scatter Plot\n",
    "3. Correlation matrix >> -correlation matrix is a table containing correlation coefficients between variables. Each cell in the table represents the correlation between two variables. \n",
    "-Matrix is nothing but tabular data in rows and columns. and for matrix uses heatmap."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25e7dc3a",
   "metadata": {},
   "source": [
    "## 1.2 How to Handle Linearity if get violated"
   ]
  },
  {
   "cell_type": "raw",
   "id": "b189896b",
   "metadata": {},
   "source": [
    "Apply a nonlinear transformation to the independent and/or dependent\n",
    "variable\n",
    "1. Log transformation\n",
    "2. Square root transformation\n",
    "3. Reciprocal transformation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1b5096b",
   "metadata": {},
   "source": [
    "# 2.small/no multicolinearity"
   ]
  },
  {
   "cell_type": "raw",
   "id": "ae0fc312",
   "metadata": {},
   "source": [
    "- Multi colinearity is a concept in which there is a high co-relation in between two indpendent variables.\n",
    "- Due to multi colinearity, it is difficult to understand the relation in between Target var. and independent variables.\n",
    "- so there should not be multicolinearity in between two independent variables.\n",
    "\n",
    "Y = m1x1 + m2x2 + c\n",
    "\n",
    "m1 is how much y changes whenever x1 changes by a unit x2 keeping  constant.\n",
    "m2 is how much y changes whenever x2 changes by a unit x1 keeping constant."
   ]
  },
  {
   "cell_type": "raw",
   "id": "a9d6c3d6",
   "metadata": {},
   "source": [
    "2. No Multicollinearity:\n",
    "    \n",
    "● Multicollinearity (or collinearity) occurs when one independent variable in a\n",
    "regression model is linearly correlated with another independent variable.\n",
    "\n",
    "● This means that an independent variable can be predicted from another\n",
    "independent variable in a regression model\n",
    "\n",
    "● Multicollinearity can be a problem in a regression model because we would not\n",
    "be able to distinguish between the individual effects of the independent variables\n",
    "on the dependent\n",
    "\n",
    "● Y = M1X1 + M2X2 + C\n",
    "\n",
    "● Coefficient M1 is the increase in Y for a unit increase in X1 while keeping X2\n",
    "constant. But since X1 and X2 are highly correlated, changes in X1 would also\n",
    "cause changes in X2 and we would not be able to see their individual effect on Y.\n",
    "\n",
    "● Multicollinearity may not affect the accuracy of the model as much. But we might\n",
    "lose reliability in determining the effects of individual features in your model – and\n",
    "that can be a problem when it comes to interpretability."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a946640",
   "metadata": {},
   "source": [
    "## 2.1 How to detect Multicollinearity"
   ]
  },
  {
   "cell_type": "raw",
   "id": "c9e23bcb",
   "metadata": {},
   "source": [
    "1. VIF (Variable Inflation Factors):\n",
    "The VIF score of an independent variable represents how well the variable is\n",
    "explained by other independent variables.\n",
    "VIF = 1 → No correlation\n",
    "VIF = 1 to 5 → Moderate correlation\n",
    "VIF >10 → High correlation\n",
    "\n",
    "2. Correlation matrix / Correlation plot\n",
    "3. Scatter plots"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdb920ea",
   "metadata": {},
   "source": [
    "## VIF >> Varience Inflation Factor"
   ]
  },
  {
   "cell_type": "raw",
   "id": "655a644f",
   "metadata": {},
   "source": [
    "VIF >> Varience Inflation Factor\n",
    "\n",
    "-with the help of (VIF) we can understand is there any multicolinieary or not in between two independent variables.\n",
    "\n",
    "-we get score and wherever score are low so that variable we can drop"
   ]
  },
  {
   "cell_type": "raw",
   "id": "0bf290fc",
   "metadata": {},
   "source": [
    "VIF determines/gives the strength of correlation between the independent variables. \n",
    "It is predicted by taking a variable and regressing it against every other variable.\n",
    "\n",
    "VIF starts from 1 ...... n there is no limit\n",
    "VIF = 1 ..... no correlation between the independent variables.\n",
    "VIF exceeds 5/10 ..... indicates high multi colinearity in between independent variables."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2cbf20b",
   "metadata": {},
   "source": [
    "## 2.1 How to handle Multicollinearity"
   ]
  },
  {
   "cell_type": "raw",
   "id": "74e86e6a",
   "metadata": {},
   "source": [
    "1. Dropping variables\n",
    "2. Combining variables"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b91e8d5",
   "metadata": {},
   "source": [
    "## Handeling the multi colinearity issue:"
   ]
  },
  {
   "cell_type": "raw",
   "id": "b502139d",
   "metadata": {},
   "source": [
    "Handeling the multi colinearity issue:\n",
    "    Dropping one of the correlated features will reduce the issue.\n",
    "2.How to handle Multicollinearity there are two ways\n",
    "2.1 Dropping variables\n",
    "2.2 Combining variables"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55c94a89",
   "metadata": {},
   "source": [
    "# These are the evaluation metrics for regression problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19d22f91",
   "metadata": {},
   "outputs": [],
   "source": [
    "MSE\n",
    "RMAE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c409d318",
   "metadata": {},
   "source": [
    "## Mean Squared Error(MSE)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "62082b26",
   "metadata": {},
   "source": [
    "MSE = sum(Ya - Yp)^2/ no. of data points = SSE/N"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6512c324",
   "metadata": {},
   "source": [
    "## Root Mean Squared Error(RMSE)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "c6958593",
   "metadata": {},
   "source": [
    "RMSE = MSE^1/2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd8fde9c",
   "metadata": {},
   "source": [
    "# Normality of Residuals"
   ]
  },
  {
   "cell_type": "raw",
   "id": "a9a8620f",
   "metadata": {},
   "source": [
    "-when the errors are normally distributed and not very much scattered then we called normality of residuals\n",
    "\n",
    "-normality is nothing but normal distribution of the errors\n",
    "\n",
    "-Error terms should follow the normal distribution pattern.\n",
    "\n",
    "-It should not be too wide or too narrow."
   ]
  },
  {
   "cell_type": "raw",
   "id": "e7a31884",
   "metadata": {},
   "source": [
    "standard deviation=1\n",
    "mean=0"
   ]
  },
  {
   "cell_type": "raw",
   "id": "7879e91d",
   "metadata": {},
   "source": [
    "3. Normality of the residuals\n",
    "\n",
    "Residuals: The difference between the actual y value and the estimated y value\n",
    "Residuals = (Ya-Yp)\n",
    "\n",
    "A normal distribution has some important properties:\n",
    "    \n",
    "1. The mean, median, and mode all represent the center of the distribution.\n",
    "2. the distribution is a bell shape. which we can say normal distibution or gaution distribution\n",
    "3. ≈68% of the data falls within 1 standard deviation of the mean, ≈95% of the data\n",
    "falls within 2 STD of the mean and ≈99.7% of the data falls within 3 STD of the\n",
    "mean"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e7d77ea",
   "metadata": {},
   "source": [
    "## 3.1 How to check normality"
   ]
  },
  {
   "cell_type": "raw",
   "id": "1fae31fb",
   "metadata": {},
   "source": [
    "1. Graphs for Normality test:\n",
    "1.1 Distribution curve, Histogram (sns. displot, sns.kdeplot)\n",
    "1.2 Q-Q or Quantile-Quantile Plot >> QQ Plot  is a scatter plot created by plotting 2 different quantiles against each other.\n",
    "1,2,3,4,5,6,7,8,9,10\n",
    "30% = 4\n",
    "80% = 9\n",
    "25% = 2.6\n",
    "1......100\n",
    "1%, 2%, ......, 99%"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3e24ba7",
   "metadata": {},
   "source": [
    "## 2. Statistical Tests for Normality(Hypothesis Testing)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "6b7b8ea4",
   "metadata": {},
   "source": [
    "2.1. Shapiro-Wilk test >> It is for normality, \n",
    "it is one of the general normality tests designed to detect all departures from the normality.\n",
    "2.2. Kolmogorov-Smirnov test\n",
    "2.3. D'Agostino's K-squared test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b192e6b",
   "metadata": {},
   "source": [
    "## 3 How to handle Normality"
   ]
  },
  {
   "cell_type": "raw",
   "id": "944d4401",
   "metadata": {},
   "source": [
    "1. Check and remove outliers\n",
    "2. Apply a nonlinear transformation to the independent and/or dependent variable\n",
    "a. Log transformation\n",
    "b. Square root transformation\n",
    "c. Reciprocal transformation"
   ]
  },
  {
   "cell_type": "raw",
   "id": "ad8d691d",
   "metadata": {},
   "source": [
    "Residuals should be normally distributed.\n",
    "\n",
    "1. kdeplot>> kernal Density Estimate Plot\n",
    "2. Hypothesis Testing\n",
    "   1. Shapiro Test >> from scipy.stats import shapiro\n",
    "   It is for normality, it is one of the general normality tests designed to detect all departures from the normality.\n",
    "   2. Ks Test >> from scipy.stats import kstest\n",
    "   3. Normal Test >> from scipy.stats import normaltest\n",
    "    \n",
    "3. Q-Q Plot >> QQ Plot  is a scatter plot created by plotting 2 different quantiles against each other.\n",
    "4. Skewness of Residuals (Skew = 0 >> Data is normally distributed.)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79717eaf",
   "metadata": {},
   "source": [
    "# Skewness"
   ]
  },
  {
   "cell_type": "raw",
   "id": "aedb6548",
   "metadata": {},
   "source": [
    "Positive Skewness : Tail of distribution is longer towards right hand side\n",
    "Negative Skewness : Tail of distribution is longer towards left hand side\n",
    "    \n",
    "Symmetrical data\n",
    "\n",
    "this range we can call -0.5 to +0.5 >> Symmetrical Distribution \n",
    "\n",
    "-1  to -0.5 >> Negative Skewness\n",
    "skew < -0.5  >> Highly Negatively skewed distribution\n",
    "\n",
    "+0.5 to 1 >> Positive Skewness\n",
    "skew > +1 >> Highly Positively skewed distribution\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d2d1140",
   "metadata": {},
   "source": [
    "## 2. Statistical Tests for Normality(Hypothesis Testing)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "93cb370f",
   "metadata": {},
   "source": [
    "Hypothesis Testing is an act in statistic whereby an analyst tests an assumption regarding a population parameter.\n",
    "The methodology by the analyst depends on the nature of the data used.\n",
    "\n",
    "Null Hypothesis      : Data is normally distributed.\n",
    "Alternate Hypothesis : Data is not normally distributed.\n",
    "\n",
    "p_val >> range between 0 to 1\n",
    "0.05 >> is the significant value of Hypothesis Testing and its denoted by alpha\n",
    "\n",
    "p_val >= 0.05 >> We are accepting the null hypothesis.\n",
    "p_val < 0.05  >> We are accepting Alternate Hypothesis.\n",
    "\n",
    "p_val >= 0.05 >> Null Hypothesis is true\n",
    "p_val < 0.05  >> Null Hypothesis is False/Alternate hypothesis is true.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efd5b43d",
   "metadata": {},
   "source": [
    "# Homoscedasticity"
   ]
  },
  {
   "cell_type": "raw",
   "id": "26fd4517",
   "metadata": {},
   "source": [
    "- Error terms should be as per the Homoscdasticity phenomenon.\n",
    "- Error terms should be almost same for all the values of independent variables.\n",
    "it means in every independent data points the error should be same,from every datapoints the distance between datapoint and best fit line near to equal same."
   ]
  },
  {
   "cell_type": "raw",
   "id": "08a16939",
   "metadata": {},
   "source": [
    "4. Homoscedasticity:\n",
    "    \n",
    "● Residuals have constant variance at every level of x. This is known as\n",
    "homoscedasticity. When this is not the case, the residuals are said to suffer\n",
    "from heteroscedasticity.\n",
    "\n",
    "● When heteroscedasticity is present in a regression analysis, the results of the\n",
    "analysis become hard to trust\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf95f838",
   "metadata": {},
   "source": [
    "## 4.1 How to Check Homoscedasticity"
   ]
  },
  {
   "cell_type": "raw",
   "id": "ea27c1e0",
   "metadata": {},
   "source": [
    "1. Scatter plot between fitted value and residual plot."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab263e4f",
   "metadata": {},
   "source": [
    "## 4.2 How to handle"
   ]
  },
  {
   "cell_type": "raw",
   "id": "94bacaee",
   "metadata": {},
   "source": [
    "1. Transform the dependent variable(Y): log transformation of the dependent\n",
    "variable\n",
    "2. Redefine the dependent variable\n",
    "3. Use weighted regression: This type of regression assigns a weight to each\n",
    "data point based on the variance of its fitted value."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a69e6e4",
   "metadata": {},
   "source": [
    "# Towards the best fit line"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0df9ed3",
   "metadata": {},
   "source": [
    "# Gradient Descend Algorithm"
   ]
  },
  {
   "cell_type": "raw",
   "id": "2daddea6",
   "metadata": {},
   "source": [
    "❖ Gradient Descent Algorithm\n",
    "\n",
    "● Gradient descent is an iterative optimization algorithm to find the minimize the\n",
    "Loss Function.\n",
    "● Gradient descent is a method of updating m and c values to minimize the cost\n",
    "function (MSE) and to get the best fit line(regression line)\n",
    "\n",
    "● Best Fit Line: When working with linear regression, our main goal is to find the\n",
    "best fit line which means the error between predicted values and actual values\n",
    "should be minimized. The best fit line will always have the least Mean squares\n",
    "error.\n",
    "\n",
    "● A regression model uses gradient descent to update the coefficients of the line\n",
    "(m and c) by reducing the cost function by a random selection of coefficient\n",
    "values and then iteratively updating the values to reach the minimum cost\n",
    "function.\n",
    "\n",
    "● To update m and c, we take gradients from the cost function. To find these\n",
    "gradients, we take partial derivatives for m and c.\n",
    "\n",
    "● Calculate the partial derivative of the loss function with respect to m, and plug in\n",
    "the current values of x, y, m, and c in it to obtain the derivative value D.\n",
    "○\n",
    "○\n",
    "● Global minima: It is a point that obtains the absolute lowest value of our function\n",
    "    \n",
    "● Learning Rate: It determines the size of the steps that are taken by the gradient\n",
    "    descent algorithm\n",
    "    \n",
    "● If α (Learning Rate) is very small, it would take a long time to converge and\n",
    "become computationally expensive.\n",
    "\n",
    "● If α is large, it may fail to converge and overshoot the minimum.\n",
    "\n",
    "● The most commonly used rates are : 0.001, 0.003, 0.01, 0.03, 0.1, 0.3."
   ]
  },
  {
   "cell_type": "raw",
   "id": "6ea61410",
   "metadata": {},
   "source": [
    "with the help of gradient dedcend algorithm we get multiple backend lines.and finally we get best fit line"
   ]
  },
  {
   "cell_type": "raw",
   "id": "059cd78f",
   "metadata": {},
   "source": [
    "y = mx + c .... line equation\n",
    "m = 0, c = 0 ....... y = 0\n",
    "m = 1, c = 0 ....... y = x"
   ]
  },
  {
   "cell_type": "raw",
   "id": "f82a6179",
   "metadata": {},
   "source": [
    "(a+b)^2 = a^2 + 2ab +b^2 ... quadratic equation."
   ]
  },
  {
   "cell_type": "raw",
   "id": "91f0b72e",
   "metadata": {},
   "source": [
    "-Gradient descent is an iterative optimization algorithm to find the minimize the\n",
    "Loss Function.\n",
    "\n",
    "● Gradient descent is a method of updating m and c values to minimize the cost\n",
    "function (MSE) and to get the best fit line(regression line)\n",
    "\n",
    "- It start with some random values of m and c\n",
    "- With the help of learning step, local minima will get plotted.\n",
    "- And the value for learning step is decided with the help of partial derivaties with the help of bold driver algorithm.\n",
    "1.Learning Step: This are not constant, they become smaller and smaller.internally bold driver algorithm works for the same.\n",
    "it is the distance between two local minimas.\n",
    "2.local minima : temperory points of multiple lines.\n",
    "3.global minima: - then it will reach to the global minima, where we do get the best values of m and c. where the error is minimum.\n",
    "- At the back end it creates multiple lines w. r. to different values of m and c.\n",
    "\n",
    "-best fit line >> the line for which we get the lowest value of sum of residual square will be our best fit line.\n",
    "   or we can say the line which passes from maximum number of datapointes.where the error between predicted values and actual      values should be minimized.\n",
    "-The best fit line will always have the least Mean squares error.\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "4b58490c",
   "metadata": {},
   "source": [
    "Y= mx + c\n",
    "line   slope  intercept   \n",
    "l1      m1        c1      >>>>> data pts are away >>> E1 ..... High\n",
    "l2      m2        c2      >>>>> data pts are bit away >>> E2 .... bit lower/ medium\n",
    ".\n",
    ".\n",
    ".\n",
    "lbf      mbf        cbf      >>>>> many data points are on the line and others are very close to the line\n",
    "                                                      >>>> Ebf..... is least"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8caf7162",
   "metadata": {},
   "source": [
    "## Gradient Descent variants"
   ]
  },
  {
   "cell_type": "raw",
   "id": "1bd8744a",
   "metadata": {},
   "source": [
    "There are three types of gradient descent methods based on the amount of data\n",
    "used to calculate the gradient:\n",
    "1. Batch gradient descent\n",
    "2. Stochastic gradient descent\n",
    "3. Mini-batch gradient descent"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5650afa",
   "metadata": {},
   "source": [
    "## calculation of error"
   ]
  },
  {
   "cell_type": "raw",
   "id": "407f3c59",
   "metadata": {},
   "source": [
    "loss function/cost function\n",
    "- The loss is error in our predicted value of m and c.\n",
    "- Our goal is to minimize the error to get best values of m and c where we will be going to get our best fit line.\n",
    "- We will use the Mean Squared Error function to calculate the loss\n",
    "E = 1/n sum(Yi- Y^)^2 ......... nothing but sum of varience.\n",
    "  = 1/n sum(Yi -(mx +c))^2"
   ]
  },
  {
   "cell_type": "raw",
   "id": "7ff259f7",
   "metadata": {},
   "source": [
    "-Total Error is distance between the actual value and the mean or expected value.\n",
    "-Residual error is distance between actual value and predicted value.\n",
    "-regression error is distance between the predicted value and mean value.\n",
    "-Error is nothing but >> simply a varience\n",
    "-Average and actual value"
   ]
  },
  {
   "cell_type": "raw",
   "id": "9b98bf64",
   "metadata": {},
   "source": [
    "P1- P2 >> Residual error>> Sum of squre residual error     >> SSE\n",
    "P2- Y^ >> Regression Error>> Sum of square regression error  >> SSR\n",
    "P1- Y^ >> Total Error>>  Sum of square total error             >> SST"
   ]
  },
  {
   "cell_type": "raw",
   "id": "038bbe7d",
   "metadata": {},
   "source": [
    "Gradient decsent algorithm will find out the best fit line (BFL), where the the error is minimum."
   ]
  },
  {
   "cell_type": "raw",
   "id": "eb25fb90",
   "metadata": {},
   "source": [
    "a. So, the model is \"most fit\" where every single data point is on line. SSE = 0\n",
    "b. SSR should be equal to SST\n",
    "   SST = SSE + SSR\n",
    "   SST = 0 + SSR\n",
    "   SST = SSR >>> SSR/SST = 1\n",
    "c. \"Poor fit\" is nothing but large SSE and SSR/SST will be close to zero."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d30a879b",
   "metadata": {},
   "source": [
    "## coeficient of Determination (R^2)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "4d52e9b0",
   "metadata": {},
   "source": [
    "coeficient of determination is denoted by R^2\n",
    "SSR/SST >> is called as R^2"
   ]
  },
  {
   "cell_type": "raw",
   "id": "7a0703a9",
   "metadata": {},
   "source": [
    "R^2 = SSR/SST = 1-(SSE/SST)\n",
    "Range for R^2 is from 0 to 1\n",
    "\n",
    "Ideal value for R^2 = 1"
   ]
  },
  {
   "cell_type": "raw",
   "id": "107ff71b",
   "metadata": {},
   "source": [
    "a. SSE is always less than SST.\n",
    "b. When SSE = 0 >> R^2 = 1 ....... Ideal case\n",
    "c. When SSE < SST >> R^2 = 0 to 1\n",
    "d. When SSE > SST >> R^2 = -ve ...... Hypothetical Situation."
   ]
  },
  {
   "cell_type": "raw",
   "id": "6be4436d",
   "metadata": {},
   "source": [
    "When BFL is worst than the average/ mean line, so in that case SSE will be greater than SST."
   ]
  },
  {
   "cell_type": "raw",
   "id": "0c697a98",
   "metadata": {},
   "source": [
    "R^2 is used to check the goodness of our best fit line. \n",
    "For features which are good predictors, R^2 value will be very high.\n",
    "For features which are bad predictors, R^2 value will be very low."
   ]
  },
  {
   "cell_type": "raw",
   "id": "3ce7f293",
   "metadata": {},
   "source": [
    "fluke relation means always change the relationship  \n",
    "R^2 is not a very good metric for evaluation of model, because it get easily impacted by fluke relationship.\n",
    "And it also get inscrease even if the feature is a bad predictor for our model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8847cda0",
   "metadata": {},
   "source": [
    "## Adjusted coeficient of Determination (R^2)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "897dc00f",
   "metadata": {},
   "source": [
    "Adjusted R^2 come into the picture as R^2 is not a very good metric for model evaluation.\n",
    "\n",
    "                        R^2           adjusted R^2\n",
    "                        0.85            0.83\n",
    "    Good Predictor      0.870           0.85\n",
    "    Bad Predictor       0.875           0.80\n",
    "    "
   ]
  },
  {
   "cell_type": "raw",
   "id": "6e01a3f7",
   "metadata": {},
   "source": [
    "Adjusted R^2 =  1-((1-R^2)(n-1))/(n-k-1)\n",
    "   n = total no.of datapoints\n",
    "   k = no. of independent variables."
   ]
  },
  {
   "cell_type": "raw",
   "id": "e24a4d64",
   "metadata": {},
   "source": [
    " 2 columns >> 1.Independent variable    \n",
    "              2.Dependent variable\n",
    " 100 % data of independent and dependent variable.\n",
    " \n",
    " Training data >> 80% >> we take 80% Independent variable and Dependent variable data\n",
    " Testing data  >> 20% >> we take 20% Independent variable data.\n",
    " \n",
    "1.seaborn >> Library for data visualization.\n",
    "2.matplotlib >> also used for data  visualization."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a9cba3d",
   "metadata": {},
   "source": [
    "##  Advantages:"
   ]
  },
  {
   "cell_type": "raw",
   "id": "db2fd124",
   "metadata": {},
   "source": [
    "1. Simple to implement and easier to interpret the output coefficients.\n",
    "2. When you know the dependent and independent variables have a linear\n",
    "relationship, this algorithm is the best to use because it’s less complex as\n",
    "compared to other algorithms.\n",
    "3. Linear Regression is prone to over-fitting but it can be avoided using some\n",
    "dimensionality reduction techniques, regularization (L1 and L2) techniques, and\n",
    "cross-validation.\n",
    "1. This is a very basic algorithm\n",
    "2. Implimentation is easy.\n",
    "3. Perform well on linearly seperable data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff70bad1",
   "metadata": {},
   "source": [
    "## Disadvantages:"
   ]
  },
  {
   "cell_type": "raw",
   "id": "58af9169",
   "metadata": {},
   "source": [
    "1. If the independent features are correlated it may affect performance.\n",
    "2. it is only efficient for linear data(High Corr between x and Y)\n",
    "3. Sometimes a lot of feature engineering is required.\n",
    "4. Scaling is Required: predictors have a mean of 0.\n",
    "5. It is often quite prone to noise and overfitting.\n",
    "6. It is sensitive to missing values.\n",
    "7. It is sensitive to Outliers\n",
    "1. There are so many assumption of LR.\n",
    "2. It often prone to overfitting issue.\n",
    "3. Any of the assumption get violated, there will be an impact on model performance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e2cb20e",
   "metadata": {},
   "source": [
    "## Applications:"
   ]
  },
  {
   "cell_type": "raw",
   "id": "68f7267b",
   "metadata": {},
   "source": [
    "1. Forecasting the data\n",
    "2. Analyzing the time series\n",
    "3. Price Prediction\n",
    "4. Salary Prediction\n",
    "1.House price prediction\n",
    "2.weather forecasting\n",
    "3.stock price prediction\n",
    "4.finance domain\n",
    "5.Insuarance domain"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29aadcf6",
   "metadata": {},
   "source": [
    "## Evaluation Metrics for Linear Regression:"
   ]
  },
  {
   "cell_type": "raw",
   "id": "98b95f82",
   "metadata": {},
   "source": [
    "1. Mean Absolute Error(MAE): It is most Robust to outliers. it means even outliers present it will give good MAE value.\n",
    "2. Mean Squared Error(MSE)\n",
    "3. Root Mean Squared Error(RMSE)\n",
    "4. R-squared or Coefficient of Determination:\n",
    "a. SSE(Sum of Squared Error\n",
    "b. SSR(Sum of Squares due to Regression)\n",
    "c. SST(Sum of Squares Total or Total Error)\n",
    "5. Adjusted R Squared"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbd9fd75",
   "metadata": {},
   "source": [
    "## 1.Mean Squared Error(MSE)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "2f937ca6",
   "metadata": {},
   "source": [
    "MSE = sum(Ya - Yp)^2/ no. of data points = SSE/N"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eefd04df",
   "metadata": {},
   "source": [
    "## Root Mean Squared Error(RMSE)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "3c63b413",
   "metadata": {},
   "source": [
    "RMSE = MSE^1/2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8df0e87c",
   "metadata": {},
   "source": [
    "## What if Assumptions will get violated"
   ]
  },
  {
   "cell_type": "raw",
   "id": "39d5660d",
   "metadata": {},
   "source": [
    "What if the assumptions will get violated ?\n",
    "\n",
    "1.Linearity:\n",
    "   -There should be Linear relationship in between independent and dependent variable.if there is not linear relationship them. >>\n",
    "    1.Model will get mislead, \n",
    "      meaning that it will result in bad values of mean squared error and R-Squared, thereby afflicting out the prediction.\n",
    "    What should we do then >> Apply a nonlinear transformation to the independent and/or dependent variable\n",
    "                              1. Log transformation\n",
    "                              2. Square root transformation\n",
    "                              3. Reciprocal transformation\n",
    "                              \n",
    "2.No/Small Multicolinearity:\n",
    "    No multicolinearity in between independent variables.\n",
    "    y = mx + c\n",
    "    y = m1x1 + m2x2 + c >> when we checking relationship with x1 and y that time x2 should be constant and when checking relationship with x2 and y x1 should be constant so we can easily understand the relation between target variable and independent variable.if this condition not satisfied means when x1 increses x2 also increses or x1 decreses x2 also decreses so it is difficult to understand the relationship between target and independent variable so>> \n",
    "    \n",
    "    1.Model will not perform well or we can say it will not perform upto the mark.\n",
    "    what should we do then>> 1. Dropping variables\n",
    "                             2. Combining variables\n",
    "    \n",
    "    \n",
    "3.Normality of residual error:\n",
    "   -Error should be normally distributed. error terms should be in noramally distributed curve.\n",
    "   -Normality of residual fail then we not getting normal distribution curve means impact on skewness. Curve may too narrow and too wide that's why impact on performance\n",
    "   \n",
    "\n",
    "4.Error should be as per the Homoscadesticity phenomenon:\n",
    "   Homo >> Same\n",
    "   Scadesticity >> Varience\n",
    "   It means equal scatter.\n",
    "   -Homoscedasticiy violated then we got hetroscedisticty means error term are not equal to the line so again impact on performance\n",
    "   If there is unequal scatter or no homoscedasticity or have heteroscedasticity, We can convert it into the homoscadesticity with the help of transformation techniques like log transformation, Box- cox Transformation.\n",
    "   A Box- Cox Transformation is a transformation of non-normal variable into a normal shape."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed96f488",
   "metadata": {},
   "source": [
    "## Common Results when Assumptions Violated"
   ]
  },
  {
   "cell_type": "raw",
   "id": "42687c45",
   "metadata": {},
   "source": [
    "- for every assumption we can say model performence will be hamper or we can say decrese.\n",
    " -There will be high difference between training and testing dataset accuracy.\n",
    "1.Predictions will be wrong. 2.mean squared error values will be bad. 3.r2 values also  will get worst >> this regarding regression\n",
    "2.model will be mislead. 2.model will bais. 3. This will be affect on prediction. >> this regarding regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9b373fe",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
