{
 "cells": [
  {
   "cell_type": "raw",
   "id": "f29c07f7",
   "metadata": {},
   "source": [
    "There are two different regularization techniques:\n",
    "    1. Ridge Regression >> L2 Regularization Technique\n",
    "    2. Lasso Regression >> L1 Regularization Technique\n",
    "\n",
    "These are mainly used to deal with the overfitting issue or we can say when there is an issue overfitting, ridge and lasso can be use.\n",
    "Overfitting >> Low bias and High Varience\n",
    "\n",
    "It generally converts high varience into low varience.\n",
    "A generalized/ good model does have low bias and low varience."
   ]
  },
  {
   "cell_type": "raw",
   "id": "9c1a0b29",
   "metadata": {},
   "source": [
    "Ridge and lasso Regression are very similar in working to linear regression.\n",
    "The only difference is the addition of l1 and l2 penalty in lasso and ridge regression respectively. penalty is nothing but lambda"
   ]
  },
  {
   "cell_type": "raw",
   "id": "5180c3d2",
   "metadata": {},
   "source": [
    "Cost function/ Loss function:\n",
    "    Mean squared error"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cd79a49",
   "metadata": {},
   "source": [
    "# Ridge Regression"
   ]
  },
  {
   "cell_type": "raw",
   "id": "112938f1",
   "metadata": {},
   "source": [
    "L2 Regularization\n",
    "Cost function : summation(Y-Ymean)^2 + lambda*(slope)^2\n",
    "Stif slope : inclination is more and it leads to overfitting\n",
    "\n",
    "Lambda >> penalty >> it is the parameter which penalize the slope.\n",
    "Lambda can be assign from 0 to any positive value.\n",
    "\n",
    "Hyper parameter tunning does to get the best value of lambda.\n",
    "Here slopes will never reach to zero"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ea11698",
   "metadata": {},
   "source": [
    "# Lasso Regression"
   ]
  },
  {
   "cell_type": "raw",
   "id": "99d9fda2",
   "metadata": {},
   "source": [
    "L1 Regularization\n",
    "\n",
    "Lasso not only helps in handelling overfitting issue, but it also does help in feature selection.\n",
    "- Here some of the slopes will reach to zero and so they can be removed or considered as less important.\n",
    "\n",
    "Cost Function : summation(Y-Ymean)^2 + lambda*|slope|\n",
    "\n",
    "50 features\n",
    "22 features slope = 0\n",
    "Final features which are important >> 28"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c6a03bf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
